from flask import Flask, jsonify, render_template, request
from openai import OpenAI

from prompt_check import PromptChecker

language = "Japanese"
translated_input = user_input


client = OpenAI(api_key="")

app = Flask(__name__)
checker = PromptChecker()


def get_response_from_chatgpt(user_input):

    # ãƒ¦ãƒ¼ã‚¶ã®å…¥åŠ›ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒã‹äº‹å‰ã«ãƒã‚§ãƒƒã‚¯
    language, translated_input = checker.forward_translate(user_input)
    if checker.is_malicious(translated_input):
        print("PROMPT INJECTION is detected. ğŸ˜¡ğŸ˜¡ğŸ˜¡")
        return "PROMPT INJECTION is detected. ğŸ˜¡ğŸ˜¡ğŸ˜¡"

    # ãƒ¦ãƒ¼ã‚¶ã®å…¥åŠ›ã‚’OpenAI APIã«æŠ•ã’ã‚‹
    response = client.chat.completions.create(model="gpt-3.5-turbo",
                                              messages=[
                                                  {"role": "assistant",
                                                      "content": translated_input},
                                              ])

    # è¿”ä¿¡ã®ã¿ã‚’å‡ºåŠ›
    llm_output = response.choices[0].message.content

    # å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã®æ¯’æ€§ãƒã‚§ãƒƒã‚¯
    if checker.is_toxic(llm_output):
        print("OUTPUT is toxic. ğŸ˜¡ğŸ˜¡ğŸ˜¡")
        return "OUTPUT is toxic. ğŸ˜¡ğŸ˜¡ğŸ˜¡"

    output = checker.backward_translate(language, llm_output)

    return output


@app.route('/')
def index():
    return render_template('index.html')


@app.route('/get_response', methods=['POST'])
def get_response():
    user_input = request.json['user_input']
    response = get_response_from_chatgpt(user_input)
    return jsonify({'response': response})


if __name__ == '__main__':
    app.run(debug=True)
